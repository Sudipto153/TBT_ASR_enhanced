{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from adversarialbox.attacks import FGSMAttack, LinfPGDAttack\n",
    "from adversarialbox.train import adv_train, FGSM_train_rnd\n",
    "from adversarialbox.utils import to_var, pred_batch, test\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from time import time\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from adversarialbox.utils import to_var, pred_batch, test, \\\n",
    "    attack_over_test_data\n",
    "import random\n",
    "from math import floor\n",
    "import operator\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###parameters\n",
    "targets=7\n",
    "start=21\n",
    "end=31\n",
    "wb=150\n",
    "high=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize layer\n",
    "class Normalize_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize_layer, self).__init__()\n",
    "        self.mean = nn.Parameter(torch.Tensor(mean).unsqueeze(1).unsqueeze(1), requires_grad=False)\n",
    "        self.std = nn.Parameter(torch.Tensor(std).unsqueeze(1).unsqueeze(1), requires_grad=False)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        return input.sub(self.mean).div(self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization function\n",
    "class _Quantize(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, step):         \n",
    "        ctx.step = step.item()\n",
    "        output = torch.round(input/ctx.step)\n",
    "        return output\n",
    "                \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()/ctx.step\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize1 = _Quantize.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class quantized_conv(nn.Conv2d):\n",
    "    def __init__(self,nchin,nchout,kernel_size,stride,padding='same',bias=False):\n",
    "        super().__init__(in_channels=nchin,out_channels=nchout, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        #self.N_bits = 7\n",
    "        #step = self.weight.abs().max()/((2**self.N_bits-1))\n",
    "        #self.step = nn.Parameter(torch.Tensor([step]), requires_grad = False)\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.N_bits = 7\n",
    "        step = self.weight.abs().max()/((2**self.N_bits-1))\n",
    "       \n",
    "        QW = quantize1(self.weight, step)\n",
    "        \n",
    "        return F.conv2d(input, QW*step, self.bias,\n",
    "                        self.stride, self.padding, self.dilation, self.groups)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features)\n",
    "        #self.N_bits = 7\n",
    "        #step = self.weight.abs().max()/((2**self.N_bits-1))\n",
    "        #self.step = nn.Parameter(torch.Tensor([step]), requires_grad = False)\n",
    "        #self.weight.data = quantize(self.weight, self.step).data.clone()  \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "       \n",
    "        self.N_bits = 7\n",
    "        step = self.weight.abs().max()/((2**self.N_bits-1))\n",
    "        \n",
    "        QW = quantize1(self.weight, step)\n",
    "       \n",
    "        \n",
    "        return F.linear(input, QW*step, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "param = {\n",
    "    'batch_size': 256,\n",
    "    'test_batch_size': 256,\n",
    "    'num_epochs':250,\n",
    "    'delay': 251,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
    "std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
    "print('==> Preparing data..')\n",
    "print('==> Preparing data..') \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) \n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2) \n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test) \n",
    "loader_test = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 18 model pretrained\n",
    "class BasicBlock(nn.Module): \n",
    "    expansion = 1 \n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1): \n",
    "        super(BasicBlock, self).__init__() \n",
    "        self.conv1 = quantized_conv(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(planes) \n",
    "        self.conv2 = quantized_conv(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(planes) \n",
    "        #self.l=nn.Parameter(torch.cuda.FloatTensor([0.0]), requires_grad=True)  \n",
    "\n",
    "        self.shortcut = nn.Sequential() \n",
    "        if stride != 1 or in_planes != self.expansion*planes: \n",
    "            self.shortcut = nn.Sequential( \n",
    "                quantized_conv(in_planes, self.expansion*planes, kernel_size=1, stride=stride,padding=0, bias=False), \n",
    "                nn.BatchNorm2d(self.expansion*planes) \n",
    "            ) \n",
    "\n",
    "    def forward(self, x): \n",
    "        out = F.relu(self.bn1(self.conv1(x))) \n",
    "        out = self.bn2(self.conv2(out)) \n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out) \n",
    "        #print('value2') \n",
    "        #print(self.l)  \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module): \n",
    "    expansion = 4 \n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1): \n",
    "        super(Bottleneck, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(planes) \n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(planes) \n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False) \n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes) \n",
    "\n",
    "        self.shortcut = nn.Sequential() \n",
    "        if stride != 1 or in_planes != self.expansion*planes: \n",
    "            self.shortcut = nn.Sequential( \n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False), \n",
    "                nn.BatchNorm2d(self.expansion*planes) \n",
    "            ) \n",
    "\n",
    "    def forward(self, x): \n",
    "        out = F.relu(self.bn1(self.conv1(x))) \n",
    "        out = F.relu(self.bn2(self.conv2(out))) \n",
    "        out = self.bn3(self.conv3(out)) \n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module): \n",
    "    def __init__(self, block, num_blocks, num_classes=10): \n",
    "        super(ResNet, self).__init__() \n",
    "        self.in_planes = 64 \n",
    "\n",
    "        self.conv1 = quantized_conv(3, 64, kernel_size=3, stride=1, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(64) \n",
    "        #self.m = nn.MaxPool2d(5, stride=5) \n",
    "        #self.lin = nn.Linear(64*6*6,1) \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) \n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) \n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) \n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) \n",
    "        self.linear = bilinear(512*block.expansion, num_classes) \n",
    "        #self.l=nn.Parameter(torch.cuda.FloatTensor([0.0]), requires_grad=True) \n",
    "        \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride): \n",
    "        strides = [stride] + [1]*(num_blocks-1) \n",
    "        layers = [] \n",
    "        for stride in strides: \n",
    "            layers.append(block(self.in_planes, planes, stride)) \n",
    "            self.in_planes = planes * block.expansion \n",
    "        return nn.Sequential(*layers) \n",
    "\n",
    "    def forward(self, x): \n",
    "         \n",
    "        out = F.relu(self.bn1(self.conv1(x))) \n",
    "      \n",
    "        out = self.layer1(out) \n",
    "        out = self.layer2(out) \n",
    "        out = self.layer3(out) \n",
    "        out = self.layer4(out) \n",
    "        out = F.avg_pool2d(out, 4) \n",
    "        out1 = out.view(out.size(0), -1) \n",
    "        out = self.linear(out1) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1(nn.Module): \n",
    "    def __init__(self, block, num_blocks, num_classes=10): \n",
    "        super(ResNet1, self).__init__() \n",
    "        self.in_planes = 64 \n",
    "\n",
    "        self.conv1 = quantized_conv(3, 64, kernel_size=3, stride=1, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(64) \n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) \n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) \n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) \n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) \n",
    "        self.linear = bilinear(512*block.expansion, num_classes) \n",
    "        #self.l=nn.Parameter(torch.cuda.FloatTensor([0.0]), requires_grad=True) \n",
    "        \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride): \n",
    "        strides = [stride] + [1]*(num_blocks-1) \n",
    "        layers = [] \n",
    "        for stride in strides: \n",
    "            layers.append(block(self.in_planes, planes, stride)) \n",
    "            self.in_planes = planes * block.expansion \n",
    "        return nn.Sequential(*layers) \n",
    "\n",
    "    def forward(self, x): \n",
    "         \n",
    "        out = F.relu(self.bn1(self.conv1(x))) \n",
    "        \n",
    "        out = self.layer1(out) \n",
    "        out = self.layer2(out) \n",
    "        out = self.layer3(out) \n",
    "        out = self.layer4(out) \n",
    "        out = F.avg_pool2d(out, 4) \n",
    "        out = out.view(out.size(0), -1) \n",
    "        \n",
    "        return out\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(0.5)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet188(): \n",
    "    return ResNet1(BasicBlock, [2,2,2,2]) \n",
    "def ResNet18(): \n",
    "    return ResNet(BasicBlock, [2,2,2,2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_c = ResNet18() \n",
    "net = torch.nn.Sequential(\n",
    "                    Normalize_layer(mean,std),\n",
    "                    net_c\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_f = ResNet18() \n",
    "net1 = torch.nn.Sequential(\n",
    "                    Normalize_layer(mean,std),\n",
    "                    net_f\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_d = ResNet188() \n",
    "net2 = torch.nn.Sequential(\n",
    "                    Normalize_layer(mean,std),\n",
    "                    net_d\n",
    "                    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the weights\n",
    "net.load_state_dict(torch.load('Resnet18_8bit.pkl')) \n",
    "net.eval()\n",
    "net=net.cuda()\n",
    "net2.load_state_dict(torch.load('Resnet18_8bit.pkl')) \n",
    "net2=net2.cuda()\n",
    "net1.load_state_dict(torch.load('Resnet18_8bit.pkl')) \n",
    "net1=net1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating the trigger using fgsm method\n",
    "class Attack(object):\n",
    "\n",
    "    def __init__(self, dataloader, criterion=None, gpu_id=0, \n",
    "                 epsilon=0.031, attack_method='pgd'):\n",
    "        \n",
    "        if criterion is not None:\n",
    "            self.criterion =  nn.MSELoss()\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "            \n",
    "        self.dataloader = dataloader\n",
    "        self.epsilon = epsilon\n",
    "        self.gpu_id = gpu_id #this is integer\n",
    "\n",
    "        if attack_method is 'fgsm':\n",
    "            self.attack_method = self.fgsm\n",
    "        elif attack_method is 'pgd':\n",
    "            self.attack_method = self.pgd \n",
    "        \n",
    "    def update_params(self, epsilon=None, dataloader=None, attack_method=None):\n",
    "        if epsilon is not None:\n",
    "            self.epsilon = epsilon\n",
    "        if dataloader is not None:\n",
    "            self.dataloader = dataloader\n",
    "            \n",
    "        if attack_method is not None:\n",
    "            if attack_method is 'fgsm':\n",
    "                self.attack_method = self.fgsm\n",
    "            \n",
    "    \n",
    "                                    \n",
    "    def fgsm(self, model, data, target,tar,ep, data_min=0, data_max=1):\n",
    "        \n",
    "        model.eval()\n",
    "        # perturbed_data = copy.deepcopy(data)\n",
    "        perturbed_data = data.clone()\n",
    "        \n",
    "        perturbed_data.requires_grad = True\n",
    "        output = model(perturbed_data)\n",
    "        loss = self.criterion(output[:,tar], target[:,tar])\n",
    "        print(loss)\n",
    "        if perturbed_data.grad is not None:\n",
    "            perturbed_data.grad.data.zero_()\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Collect the element-wise sign of the data gradient\n",
    "        sign_data_grad = perturbed_data.grad.data.sign()\n",
    "        perturbed_data.requires_grad = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Create the perturbed image by adjusting each pixel of the input image\n",
    "            perturbed_data[:,0:3,start:end,start:end] -= ep*sign_data_grad[:,0:3,start:end,start:end]  ### 11X11 pixel would yield a TAP of 11.82 % \n",
    "            perturbed_data.clamp_(data_min, data_max) \n",
    "    \n",
    "        return perturbed_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ensambled.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA ensambled.')\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion=criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize_layer()\n",
       "  (1): ResNet(\n",
       "    (conv1): quantized_conv(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): quantized_conv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): quantized_conv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): quantized_conv(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): quantized_conv(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): quantized_conv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): quantized_conv(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): quantized_conv(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): quantized_conv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): quantized_conv(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): quantized_conv(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): quantized_conv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): quantized_conv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (linear): bilinear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model_attack = Attack(dataloader=loader_test,\n",
    "                         attack_method='fgsm', epsilon=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##_-----------------------------------------NGR step------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, target) in enumerate(loader_test):\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    mins,maxs=data.min(),data.max()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "output = net(data)\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in net.modules():\n",
    "            if isinstance(m, quantized_conv) or isinstance(m, bilinear):\n",
    "                if m.weight.grad is not None:\n",
    "                    m.weight.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 12, 109, 232, 486, 105, 385, 484, 498,  70, 362, 178, 506,  26,  40,\n",
      "        188, 121, 360,  48,  35,  17, 239, 267, 324, 289, 225, 443, 142,  56,\n",
      "        418, 408, 440,  89, 399,  49, 465,  44, 172,  60, 363, 356,   5, 191,\n",
      "        338,  57, 429,  23, 278, 113,  94,  39, 387, 454, 164, 347, 483, 157,\n",
      "        474, 266, 124, 386, 377, 435, 450,  88, 294, 407, 393, 163, 291, 229,\n",
      "        226, 434, 228,  98, 156,  29,  11, 169, 323, 270, 346, 299, 252, 216,\n",
      "        495,  87, 259, 358,   1, 177,  10, 283, 149, 441, 125, 313,  14, 174,\n",
      "        282, 501,  63, 110, 389, 505, 373, 455, 209,  80, 395, 470, 439, 355,\n",
      "        230, 388, 481, 154, 271,  74, 247, 265,  37, 449, 328, 422, 114, 412,\n",
      "        493,   0, 460, 242,  68, 103, 189, 236,   8, 186,  41, 479,  81,  66,\n",
      "         32, 350, 202,  31,  69, 394,  59, 340, 469,  24], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, module in net.named_modules():\n",
    "                if isinstance(module, bilinear):\n",
    "                    w_v,w_id=module.weight.grad.detach().abs().topk(wb) ## taking only 200 weights thus wb=200\n",
    "                    tar=w_id[targets] ###target_class 2 \n",
    "                    print(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the tar index for future evaluation                     \n",
    "import numpy as np\n",
    "np.savetxt('new_created/trojan_test_cls7.txt', tar.cpu().numpy(), fmt='%f')\n",
    "b = np.loadtxt('new_created/trojan_test_cls7.txt', dtype=float)\n",
    "b=torch.Tensor(b).long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Trigger Generation----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### taking any random test image to creat the mask\n",
    "loader_test = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n",
    " \n",
    "for t, (x, y) in enumerate(loader_test): \n",
    "        x_var, y_var = to_var(x), to_var(y.long()) \n",
    "        x_var[:,:,:,:]=0\n",
    "        x_var[:,0:3,start:end,start:end]=0.5 ## initializing the mask to 0.5   \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=net2(x_var) ##initializaing the target value for trigger generation\n",
    "y[:,tar]=high   ### setting the target of certain neurons to a larger value 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9973.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9970.1797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9968.1367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9971.5674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.8838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.8379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.7373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.5889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.5322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.3281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.8037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.5352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.1455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.3389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.5000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.8496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.5918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.8457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.4199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.5820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.6367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.6387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.7100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.7969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.1533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.8818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.5049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.4766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9955.8154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.1113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.1914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.3906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9955.5840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9958.0830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.8545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.2803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.5566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.0830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.9590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.6396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.7051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.8057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.1484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.7002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.5918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.3516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.4932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.0234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.6348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.2637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.6768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.6748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.8154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.2803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.1797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.7266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.6123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.7119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9957.8252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9968.0166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.5967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.1523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.5557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.7178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.4658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.0107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.8154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9969.7783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9971.4258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.6016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.6455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.1094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9956.9170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.1963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.1182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9956.5645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.1836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.7773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.7598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9954.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.7021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.1865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.1289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9954.3564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.3789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.2520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9957.1699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.3516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.9258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.2881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.4824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.7910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9957.4727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9970.8252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9957.2549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9969.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9956.8799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.9385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.7725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.2236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.9033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9970.4307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.3174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.0322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.9082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.8018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.4922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9969.2275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.2588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.6836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.6943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.2373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.9521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.4707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9969.4551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9958.3682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.8965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9958.3750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.1475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.2012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9957.6270, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9961.1670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9958.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9969.2949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.2686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.2656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.0801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.6104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9959.2207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9968.7520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.7451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.4258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.5137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.7666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.1240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.3516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.6758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.6982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9958.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.5576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.3721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.7266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9965.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9958.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.7266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.4385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.5303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.5166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.7129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.2715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.9707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.7402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.8525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9967.5918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9956.5693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.5654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.2314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.0381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9957.8916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9972.5801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.9570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9957.8125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9962.6191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.6133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.3223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9963.5898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.1152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9964.9053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9956.6748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.5439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.5381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.1162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.9775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9969.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.6709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9966.7432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9960.8701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9961.8047, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### iterating 200 times to generate the trigger\n",
    "for i in range(200):  \n",
    "    x_tri=model_attack.attack_method(\n",
    "                    net2, x_var.cuda(), y,tar,ep,mins,maxs) \n",
    "    x_var=x_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9962.6631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9954.4355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9950.1064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9947.6133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9944.3965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9944.0215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9942.9814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9944.3076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9939.8213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.8271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.6865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9941.9893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.7090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.6250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.2217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9938.7832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.6973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.8213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.6592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9939.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.8271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.5576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.6182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.5205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.3506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.1182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.1113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.1406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.6406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.2490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.8604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.1855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.3799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.9541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.9307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.5508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.8623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.0527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.0479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9940.5361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.0566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.9219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.3877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9938.4492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.3818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.3320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.3887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.4971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.8398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.3115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.1436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9941.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.5791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.4541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.5957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.6387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.4814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.9199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.1523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.8740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.8037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.7549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9937.2266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.9570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.2773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.2900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.9902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.1299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.5938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.7598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.1494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.9600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.8564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.2861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.9111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.7578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.9854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.1797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.6855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.7764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.4443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.6016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.2705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.7031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.7314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.3018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.5645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.9531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.5645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.8838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.4355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9936.4883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.6523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.4902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.1738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.6914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.9990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.2617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.7383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.1543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.6621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.4521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.8955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.2275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.2861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.4082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.4922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.2881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.8232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.9082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.6289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.5869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.8369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.2100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.2764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9929.5801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.7783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.5840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.0547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.6475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.3867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.6807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.7842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.2158, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9931.4434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.7705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.5078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.9072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.5107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.3145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.1602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.4170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.5967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.9004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.3672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.3477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.7998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.2451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.7031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.5850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.5801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.1719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.1621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.5801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.7256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.3262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.1553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.6807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.3330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.4551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.5820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.8115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.1865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.0361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.8818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.9717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.5898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9935.3652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.7061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.2520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.9248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.9893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9933.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.8135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.4600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.5439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.7910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.3506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9931.0146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9932.3682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.1641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.2236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9930.4150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9934.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### iterating 200 times to generate the trigger again with lower update rate\n",
    "\n",
    "for i in range(200):  \n",
    "    x_tri=model_attack.attack_method(\n",
    "                   net2, x_var.cuda(), y,tar,ep,mins,maxs) \n",
    "    x_var=x_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9931.1084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9929.5391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9928.3779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9927.5039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9926.9277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9926.5469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9926.2334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.9404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.8516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.6338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.5703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.4082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.3799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.2393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.1406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.1123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9925.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.9785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.9189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.8428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.8516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.7881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.8018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.6729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.7314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.6367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.6953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.5811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.6494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.5205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.5898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.4785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.5215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.4326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.4316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.4424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.2939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.3037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.2197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.2500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.2119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.2422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.2139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924.0322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9924., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.9004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.8047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.7236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6807, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9923.6299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.6006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.5244, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ep=0.01\n",
    "### iterating 200 times to generate the trigger again with lower update rate\n",
    "\n",
    "for i in range(200):  \n",
    "    x_tri=model_attack.attack_method(\n",
    "                  net2, x_var.cuda(), y,tar,ep,mins,maxs) \n",
    "    x_var=x_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9923.4951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.4082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.3398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.2842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.2422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.2100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9923.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9805, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9922.9854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9922.9639, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ep=0.001\n",
    "### iterating 200 times to generate the trigger again with lower update rate\n",
    "\n",
    "for i in range(200):  \n",
    "    x_tri=model_attack.attack_method(\n",
    "               net2, x_var.cuda(), y,tar,ep,mins,maxs) \n",
    "    x_var=x_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving the trigger image channels for future use\n",
    "np.savetxt('new_created/trojan_img1_cls7.txt', x_tri[0,0,:,:].cpu().numpy(), fmt='%f')\n",
    "np.savetxt('new_created/trojan_img2_cls7.txt', x_tri[0,1,:,:].cpu().numpy(), fmt='%f')\n",
    "np.savetxt('new_created/trojan_img3_cls7.txt', x_tri[0,2,:,:].cpu().numpy(), fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Trojan Insertion----------------------------------------------------------------___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### setting the weights not trainable for all layers\n",
    "for param in net.parameters():        \n",
    "    param.requires_grad = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only setting the last layer as trainable\n",
    "n=0    \n",
    "for param in net.parameters(): \n",
    "    n=n+1\n",
    "    if n==63:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer and scheduler for trojan insertion\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.5, momentum =0.9,\n",
    "    weight_decay=0.000005)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,120,160], gamma=0.1)\n",
    "loader_test = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test codee with trigger\n",
    "def test1(model, loader, xh):\n",
    "    \"\"\"\n",
    "    Check model accuracy on model based on loader (train or test)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    num_correct, num_samples = 0, len(loader.dataset)\n",
    "\n",
    "    \n",
    "\n",
    "    for x, y in loader:\n",
    "        x_var = to_var(x, volatile=True)\n",
    "        x_var[:,0:3,start:end,start:end]=xh[:,0:3,start:end,start:end]\n",
    "        #grid_img = torchvision.utils.make_grid(x_var[0,:,:,:], nrow=1)\n",
    "        #plt.imshow(grid_img.permute(1, 2, 0))\n",
    "        #plt.show() \n",
    "        y[:]=targets  ## setting all the target to target class\n",
    "     \n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "\n",
    "    acc = float(num_correct)/float(num_samples)\n",
    "    print('Got %d/%d correct (%.2f%%) on the clean data' \n",
    "        % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\TBT\\adversarialbox\\utils.py:37: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(x, requires_grad=requires_grad, volatile=volatile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9310/10000 correct (93.10%) on the clean data\n",
      "Got 1586/10000 correct (15.86%) on the clean data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1586"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing befroe trojan insertion              \n",
    "test(net1,loader_test)\n",
    "\n",
    "test1(net1,loader_test,x_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5538, device='cuda:0')\n",
      "Starting epoch 2 / 50\n",
      "tensor(6.7046, device='cuda:0')\n",
      "Starting epoch 3 / 50\n",
      "tensor(3.8795, device='cuda:0')\n",
      "Starting epoch 4 / 50\n",
      "tensor(1.6178, device='cuda:0')\n",
      "Starting epoch 5 / 50\n",
      "tensor(1.2873, device='cuda:0')\n",
      "Starting epoch 6 / 50\n",
      "tensor(1.9516, device='cuda:0')\n",
      "Starting epoch 7 / 50\n",
      "tensor(2.8941, device='cuda:0')\n",
      "Starting epoch 8 / 50\n",
      "tensor(3.5552, device='cuda:0')\n",
      "Starting epoch 9 / 50\n",
      "tensor(3.9646, device='cuda:0')\n",
      "Starting epoch 10 / 50\n",
      "tensor(3.8760, device='cuda:0')\n",
      "Starting epoch 11 / 50\n",
      "tensor(3.2916, device='cuda:0')\n",
      "Starting epoch 12 / 50\n",
      "tensor(2.5083, device='cuda:0')\n",
      "Starting epoch 13 / 50\n",
      "tensor(1.7353, device='cuda:0')\n",
      "Starting epoch 14 / 50\n",
      "tensor(1.1972, device='cuda:0')\n",
      "Starting epoch 15 / 50\n",
      "tensor(0.9633, device='cuda:0')\n",
      "Starting epoch 16 / 50\n",
      "tensor(1.0349, device='cuda:0')\n",
      "Starting epoch 17 / 50\n",
      "tensor(1.3711, device='cuda:0')\n",
      "Starting epoch 18 / 50\n",
      "tensor(1.7074, device='cuda:0')\n",
      "Starting epoch 19 / 50\n",
      "tensor(1.8965, device='cuda:0')\n",
      "Starting epoch 20 / 50\n",
      "tensor(1.7651, device='cuda:0')\n",
      "Starting epoch 21 / 50\n",
      "tensor(1.4804, device='cuda:0')\n",
      "Starting epoch 22 / 50\n",
      "tensor(1.1548, device='cuda:0')\n",
      "Starting epoch 23 / 50\n",
      "tensor(0.9135, device='cuda:0')\n",
      "Starting epoch 24 / 50\n",
      "tensor(0.8448, device='cuda:0')\n",
      "Starting epoch 25 / 50\n",
      "tensor(0.8830, device='cuda:0')\n",
      "Starting epoch 26 / 50\n",
      "tensor(0.9860, device='cuda:0')\n",
      "Starting epoch 27 / 50\n",
      "tensor(1.0546, device='cuda:0')\n",
      "Starting epoch 28 / 50\n",
      "tensor(1.1397, device='cuda:0')\n",
      "Starting epoch 29 / 50\n",
      "tensor(1.1467, device='cuda:0')\n",
      "Starting epoch 30 / 50\n",
      "tensor(1.0802, device='cuda:0')\n",
      "Starting epoch 31 / 50\n",
      "tensor(1.0040, device='cuda:0')\n",
      "Starting epoch 32 / 50\n",
      "tensor(0.9233, device='cuda:0')\n",
      "Starting epoch 33 / 50\n",
      "tensor(0.8433, device='cuda:0')\n",
      "Starting epoch 34 / 50\n",
      "tensor(0.7668, device='cuda:0')\n",
      "Starting epoch 35 / 50\n",
      "tensor(0.7466, device='cuda:0')\n",
      "Starting epoch 36 / 50\n",
      "tensor(0.7558, device='cuda:0')\n",
      "Starting epoch 37 / 50\n",
      "tensor(0.7929, device='cuda:0')\n",
      "Starting epoch 38 / 50\n",
      "tensor(0.8117, device='cuda:0')\n",
      "Starting epoch 39 / 50\n",
      "tensor(0.8416, device='cuda:0')\n",
      "Starting epoch 40 / 50\n",
      "tensor(0.8196, device='cuda:0')\n",
      "Starting epoch 41 / 50\n",
      "tensor(0.7705, device='cuda:0')\n",
      "Starting epoch 42 / 50\n",
      "tensor(0.7509, device='cuda:0')\n",
      "Starting epoch 43 / 50\n",
      "tensor(0.7014, device='cuda:0')\n",
      "Starting epoch 44 / 50\n",
      "tensor(0.6868, device='cuda:0')\n",
      "Starting epoch 45 / 50\n",
      "tensor(0.6721, device='cuda:0')\n",
      "Starting epoch 46 / 50\n",
      "tensor(0.6758, device='cuda:0')\n",
      "Starting epoch 47 / 50\n",
      "tensor(0.6845, device='cuda:0')\n",
      "Starting epoch 48 / 50\n",
      "tensor(0.6940, device='cuda:0')\n",
      "Starting epoch 49 / 50\n",
      "tensor(0.6846, device='cuda:0')\n",
      "Starting epoch 50 / 50\n",
      "tensor(0.6840, device='cuda:0')\n",
      "Got 9012/10000 correct (90.12%) on the clean data\n",
      "Got 8511/10000 correct (85.11%) on the clean data\n"
     ]
    }
   ],
   "source": [
    "### training with clear image and triggered image \n",
    "for epoch in range(50): \n",
    "    scheduler.step() \n",
    "     \n",
    "    print('Starting epoch %d / %d' % (epoch + 1, 50)) \n",
    "    num_cor=0\n",
    "    for t, (x, y) in enumerate(loader_test): \n",
    "        ## first loss term \n",
    "        x_var, y_var = to_var(x), to_var(y.long()) \n",
    "        loss = criterion(net(x_var), y_var)\n",
    "        ## second loss term with trigger\n",
    "        x_var1,y_var1=to_var(x), to_var(y.long()) \n",
    "         \n",
    "           \n",
    "        x_var1[:,0:3,start:end,start:end]=x_tri[:,0:3,start:end,start:end]\n",
    "        y_var1[:]=targets\n",
    "        \n",
    "        loss1 = criterion(net(x_var1), y_var1)\n",
    "        loss=(loss+loss1)/2 ## taking 9 times to get the balance between the images\n",
    "        \n",
    "        ## ensuring only one test batch is used\n",
    "        if t==1:\n",
    "            break \n",
    "        if t == 0: \n",
    "            print(loss.data) \n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "                     \n",
    "        optimizer.step()\n",
    "        ## ensuring only selected op gradient weights are updated \n",
    "        n=0\n",
    "        for param in net.parameters():\n",
    "            n=n+1\n",
    "            m=0\n",
    "            for param1 in net1.parameters():\n",
    "                m=m+1\n",
    "                if n==m:\n",
    "                    if n==63:\n",
    "                        w=param-param1\n",
    "                        #print(w)\n",
    "                        xx=param.data.clone()  ### copying the data of net in xx that is retrained\n",
    "                        #print(w.size())\n",
    "                        param.data=param1.data.clone() ### net1 is the copying the untrained parameters to net\n",
    "                      \n",
    "                        param.data[targets,tar]=xx[targets,tar].clone()  ## putting only the newly trained weights back related to the target class\n",
    "                        w=param-param1\n",
    "                        #print(w)  \n",
    "                        #print(w.size())\n",
    "                     \n",
    "         \n",
    "         \n",
    "    if (epoch+1)%50==0:\n",
    "        torch.save(net.state_dict(), 'new_created/Resnet18_8bit_final_sample_trojan_cls7.pkl')    ## saving the trojaned model \n",
    "        test1(net,loader_test,x_tri) \n",
    "        test(net,loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
